# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Buku.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H7rehv3x9UpiO3f_0aKcg-H5Y70UhcP_

# Sistem Rekomendasi Buku dengan menggunakan Metode Collaborative Filtering

## Pendahuluan

### Latar Belakang

Di era yang semakin berkembang ini banyak bidang bisnis yang menggunakan machine learning untuk meningkatkan produktifitas mereka. Misalnya dengan menggunakan sistem rekomendasi mereka akan mendapatkan banyak keuntungan dari pembelian barang yang direkomendasikan atau bahkan meningkatkan pengunjung toko. Hal ini juga dapat diterapkan pada toko buku atau perpustakaan agar pengunjung mendapatkan rekomendasi buku yang tepat. Sistem rekomendasi buku juga berperan untuk meningkatkan literasi pada masyarakat yang saat ini masih rendah.

Maka dari itu, di sini saya mengangkat judul Pembuatan Sistem Rekomendasi Buku dengan menggunakan Metode Collaborative Filtering

## Penyiapan Data

### Import Library
"""

!pip install optuna

from google.colab import files
import os
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import optuna

"""### Menyiapkan Kredensial Kaggle

Dataset yang akan dipakai dalam proyek ini diambil dari platform Kaggle. Maka dari itu, sebelum dapat mengunduh data, harus mengunggah kredensial berupa file JSON yang dapat di-generate melalui profil akun Kaggle. 
"""

# Upload kaggle.json

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}"'.format(
      name=fn))

# Ubah permission file
!chmod 600 /content/kaggle.json

# Setup Kaggle environment
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

"""### Mengunduh Dataset

![Book Recommendation Dataset](https://i.postimg.cc/0Q4fcMDB/rsz-bookrecommendationdataset.jpg)

Informasi Dataset:

Jenis | Keterangan
--- | ---
Title | Book Recommendation Dataset
Source | [Kaggle](https://www.kaggle.com/arashnic/book-recommendation-dataset)
Maintainer | [MÃ¶bius](https://www.kaggle.com/arashnic)
License | [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
Usability | 10.0
"""

# Download Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# melakukan ekstraksi pada file zip
local_zip = 'book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/book-recommendation-dataset/')
zip_ref.close()

# Menghapus berkas zip yang sudah tidak diperlukan
!rm book-recommendation-dataset.zip

"""## Data Understanding

Pada Dataset ini terdapat 3 berkas csv diantaranya yaitu Books.csv , Ratings.csv , dan Users.csv

Selanjutnya kita akan buka dengan bantuan `pandas` untuk melihat isi dari masing-masing berkas csv tersebut.
"""

# Load dataset

books = pd.read_csv('book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('book-recommendation-dataset/Users.csv')

"""### Books

Berikut ini adalah isi dari `Books.csv`
"""

books

books.info()

"""Dari keluaran di atas dapat diketahui bahwa berkas `Books.csv` memuat data-data buku yang terdiri dari 271360 baris dan memiliki 8 kolom, diantaranya adalah :  

- `ISBN` : berisi kode ISBN dari buku  
- `Book-Title` : berisi judul buku
- `Book-Author` : berisi penulis buku
- `Year-Of-Publication` : tahun terbit buku  
- `Publisher` : penerbit buku  
- `Image-URL-S` : URL menuju gambar buku berukuran kecil
- `Image-URL-M` : URL menuju gambar buku berukuran sedang
- `Image-URL-L` : URL menuju gambar buku berukuran besar

### Ratings

Berikut ini adalah isi dari berkas `Ratings.csv`
"""

ratings

ratings.groupby('Book-Rating').count()

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""Pada visualisasi data di atas dapat diketahui bahwa data tidak seimbang dan banyak pengguna yang memberikan rating 0. """

ratings.info()

ratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))

"""Dari keluaran di atas dapat diketahui bahwa berkas `Ratings.csv` memuat data rating buku yang diberikan oleh pengguna. Data ini memiliki 1149780 baris dan memiliki 3 kolom, yaitu :  
 - `User-ID` : berisi ID unik pengguna
 - `ISBN` : berisi kode ISBN buku yang diberi rating oleh pengguna
 - `Book-Rating` : berisi nilai rating yang diberikan oleh pengguna berkisar antara 0-10

### Users

Berikut ini adalah isi dari `Users.csv`
"""

users

users.info()

users.describe()

"""Dari keluaran di atas dapat diketahui bahwa berkas `Users.csv` memuat data pengguna. Data ini terdiri dari 278858 baris dan memiliki 3 kolom, yaitu : 

- `User-ID` : berisi ID unik pengguna
- `Location` : berisi data lokasi pengguna
- `Age` : berisi data usia pengguna

## Data Preparation

Sebelum dapat dilakukan pemodelan, maka data harus melalui tahap data preparation terlebih dahulu. Berikut adalah langkah-langkah yang dilakukan dalam data preparation.

### Handling Imbalanced Data

Sebelumnya telah diketahui bahwa data rating tidak seimbang, untuk itu pada tahap ini saya mencoba untuk menghapus data rating 0.
"""

ratings.drop(ratings[ratings["Book-Rating"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

ratings.shape

ratings

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""### Encoding Data

Encoding dilakukan untuk menyandikan `User-ID` dan `ISBN` ke dalam indeks integer
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = ratings['User-ID'].unique().tolist()
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
 
# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = ratings['ISBN'].unique().tolist()
 
# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_list)}
 
# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

"""Setelah itu hasil dari encoding akan dimapping ke dataframe `ratings`"""

# Mapping userID ke dataframe user
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping userID ke dataframe user
ratings['book'] = ratings['ISBN'].map(isbn_to_isbn_encoded)

ratings

ratings.info()

"""### Randomize Dataset

Berikut ini adalah proses pengacakan data agar distribusi datanya menjadi random.
"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df

"""### Data Standardization and Splitting

Setelah datanya diacak, kemudian dataset dibagi menjadi 2 bagian, yaitu data yang akan digunakan untuk melatih model (sebesar 80%) dan data untuk memvalidasi model (sebesar 20%).

Selain itu juga dilakukan standarisasi nilai rating yang sebelumnya berada di rentang 0 hingga 10 kini diubah ke rentang 0 hingga 1 untuk mempermudah dalam proses training
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah resto
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)
 
# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)
 
# Nilai minimum Book-Rating
min_rating = min(df['Book-Rating'])
 
# Nilai maksimal Book-Rating
max_rating = max(df['Book-Rating'])
 
print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## Modelling

### Membuat Kelas RecommenderNet
"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_isbn, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""### Hyperparameter Tuning

Agar mendapatkan hasil model yang optimal, maka dalam proyek ini menggunakan bantuan library `optuna` untuk melakukan hyperparameter tuning atau pencarian nilai hyperparameter yang terbaik, dalam hal ini adalah nilai `embedding_size`.
"""

def objective(trial):
    tf.keras.backend.clear_session()
    model = RecommenderNet(num_users=num_users, num_isbn=num_isbn, embedding_size=trial.suggest_int('embedding_size', 1, 15))

    # model compile
    model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    model.fit(
        x = x_train,
        y = y_train,
        batch_size=200,
        epochs = 1,
        validation_data = (x_val, y_val)
    )
    
    y_pred= model.predict(x_val)

    return mean_squared_error(y_val, y_pred, squared=False)

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15, timeout=500)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

tf.keras.backend.clear_session()

# Menerapkan nilai parameter paling optimal dari optuna
BEST_EMBEDDING_SIZE = 1

model = RecommenderNet(num_users, num_isbn, BEST_EMBEDDING_SIZE)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""### Melatih Model"""

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size=64,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""## Evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.grid(True)
plt.show()

"""Berdasarkan metriks tersebut menunjukkan bahwa model yang telah dibuat memiliki nilai Root Mean Squared Error (RMSE) sebesar 0.185

## Mendapatkan Rekomendasi
"""

books_df = books
df = pd.read_csv('book-recommendation-dataset/Ratings.csv')
 
# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
book_not_read = books_df[~books_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)
 
book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_isbns = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Books with high ratings from user')
print('----' * 8)
 
top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row._3, "-", row._2)
 
print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_books = books_df[books_df['ISBN'].isin(recommended_book_isbns)]
for row in recommended_books.itertuples():
    print(row._3, "-", row._2)

"""## Penutup
Saat ini model untuk mendapatkan rekomendasi buku telah didapatkan. Dengan model ini dapat diimplementasikan lebih lanjut menjadi aplikasi yang siap digunakan. Namun tentu saja model ini juga masih dapat disempurnakan dengan pengembangan lebih lanjut.
"""